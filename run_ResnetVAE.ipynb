{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "#\n",
    "from dotted_dict import DottedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#\n",
    "import numpy as np\n",
    "import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbones import *\n",
    "from models.projectors import *\n",
    "from models.barlow_twins import BarlowTwins\n",
    "from optimizers import *\n",
    "from augmentations import SimSiamAugmentation, Augmentation\n",
    "from datasets import get_dataset\n",
    "from utils import show, show_batch, save_checkpoint\n",
    "from config_utils import get_dataloaders_from_config, get_config_template, add_paths_to_confg\n",
    "from train_utils import down_knn, down_train_linear, down_valid_linear, std_cov_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_template()\n",
    "\n",
    "#################\n",
    "# DVICE\n",
    "#################\n",
    "config.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#################\n",
    "# frequencies\n",
    "#################\n",
    "config.freqs = {\n",
    "    \"ckpt\": 20,\n",
    "    \"lin_eval\": 5,\n",
    "    \"knn_eval\": 5,\n",
    "    \"std_eval\": 5,\n",
    "    \"plot_rec\": 20\n",
    "}\n",
    "#################\n",
    "# data\n",
    "#################\n",
    "config.p_data = \"/mnt/data/pytorch\"\n",
    "config.dataset = \"cifar10\"\n",
    "config.img_size = 64\n",
    "config.n_classes = 10\n",
    "config.train_split = 'train'\n",
    "config.down_train_split = 'train'\n",
    "config.down_valid_split = \"valid\"\n",
    "config.augmentations_train = [\n",
    "    (\"RandomResizedCrop\", {'size': config.img_size, \"scale\": (0.2, 1.0)}),\n",
    "    (\"RandomHorizontalFlip\", {'p': 0.5}),\n",
    "    (\"RandomApply\", {\n",
    "        \"transforms\": [\n",
    "            (\"ColorJitter\", {\"brightness\": 0.3,\n",
    "                             \"contrast\": 0.3,\n",
    "                             \"saturation\": 0.1,\n",
    "                             'hue': 0.1})\n",
    "        ],\n",
    "        \"p\": 0.4,\n",
    "    }),\n",
    "    #(\"RandomGrayscale\", {\"p\": 0.1}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#\n",
    "config.augmentations_valid = [\n",
    "    (\"Resize\", {'size': (config.img_size, config.img_size)}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#################\n",
    "# train model\n",
    "#################\n",
    "config.backbone =  \"ResNet-18\"\n",
    "config.projector_args = {\n",
    "    'd_out': 768,\n",
    "    'd_hidden': 768,\n",
    "    'n_hidden': 2,\n",
    "    'normalize': True,\n",
    "    'dropout_rate': None,\n",
    "    'activation_last': False,\n",
    "    'normalize_last': False,\n",
    "    'dropout_rate_last': None,\n",
    "}\n",
    "#################\n",
    "# training\n",
    "#################\n",
    "config.batch_size = 256\n",
    "config.num_epochs = 800\n",
    "config.num_workers = 8\n",
    "\n",
    "#################\n",
    "# optimizer\n",
    "#################\n",
    "#config.optimizer = \"sgd\"\n",
    "#config.optimizer_args = {\n",
    "#        \"lr\": 1e-2,\n",
    "#        \"weight_decay\": 1e-6,\n",
    "#        \"momentum\": 0.9\n",
    "#    }\n",
    "config.optimizer = \"adam\"\n",
    "config.optimizer_args = {\n",
    "    'lr': 1e-3\n",
    "}\n",
    "config.scheduler = None #\"cosine_decay\"\n",
    "config.scheduler_args = {\n",
    "        \"T_max\": config.num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "#################\n",
    "# down train\n",
    "#################\n",
    "config.down_batch_size = 512\n",
    "config.down_num_epochs = 1\n",
    "config.down_num_workers = 8\n",
    "\n",
    "#################\n",
    "# down optimizer\n",
    "#################\n",
    "config.down_optimizer = \"sgd\"\n",
    "config.down_optimizer_args = {\n",
    "        \"lr\": 0.03 * config.down_batch_size / 256,\n",
    "        \"weight_decay\": 5e-4,  # used always\n",
    "        \"momentum\": 0.9\n",
    "    }\n",
    "config.down_scheduler = \"cosine_decay\"\n",
    "config.down_scheduler_args = {\n",
    "        \"T_max\": config.down_num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "\n",
    "config.loss = {\n",
    "    'lmda_rec': 100,\n",
    "    'lmda_kld': 0.5\n",
    "}\n",
    "config.debug = False\n",
    "config.p_base = \"/mnt/experiments/siamesevae\"\n",
    "add_paths_to_confg(config)\n",
    "config = DottedDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META VARS\n",
    "P_CKPT = None\n",
    "CONTINUE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P_CKPT is not None:\n",
    "    print(\"LOADING CHECKPOINT {}\".format(P_CKPT))\n",
    "    ckpt = torch.load(P_CKPT)\n",
    "    \n",
    "    if CONTINUE:\n",
    "        print(\"USING CKPT Config\")\n",
    "        config = ckpt[\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_z, d_out, fc_hidden1=768, fc_hidden2=768):\n",
    "        super(Decoder, self).__init__()\n",
    "        #\n",
    "        self.d_out = d_out\n",
    "        self.d_z = d_z\n",
    "        self.fc_hidden1=fc_hidden1,\n",
    "        self.fc_hidden2=fc_hidden1\n",
    "        #\n",
    "        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n",
    "        self.k1, self.k2, self.k3, self.k4 = (3, 3), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
    "\n",
    "         # Sampling vector\n",
    "        self.fc4 = nn.Linear(self.d_z, self.fc_hidden2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(self.fc_hidden2)\n",
    "        self.fc5 = nn.Linear(self.fc_hidden2, 64 * 4 * 4)\n",
    "        self.fc_bn5 = nn.BatchNorm1d(64 * 4 * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.convTrans6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2,\n",
    "                               padding=0),\n",
    "            nn.BatchNorm2d(32, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.convTrans7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2,\n",
    "                               padding=0),\n",
    "            nn.BatchNorm2d(16, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.convTrans8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2,\n",
    "                               padding=0),\n",
    "            nn.BatchNorm2d(8, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.convTrans9 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=3, stride=2,\n",
    "                               padding=0),\n",
    "            #nn.BatchNorm2d(3, momentum=0.01),\n",
    "            nn.Sigmoid()    # y = (y1, y2, y3) \\in [0 ,1]^3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc_bn4(self.fc4(x)))\n",
    "        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n",
    "        x = self.convTrans6(x)\n",
    "        x = self.convTrans7(x)\n",
    "        x = self.convTrans8(x)\n",
    "        x = self.convTrans9(x)\n",
    "        x = F.interpolate(x, size=(self.d_out, self.d_out), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "    \n",
    "    def forward_no_resize(self, x):\n",
    "        x = self.relu(self.fc_bn4(self.fc4(x)))\n",
    "        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n",
    "        x = self.convTrans6(x)\n",
    "        x = self.convTrans7(x)\n",
    "        x = self.convTrans8(x)\n",
    "        x = self.convTrans9(x)\n",
    "        return x\n",
    "\n",
    "class ResnetVAE(torch.nn.Module):\n",
    "    def __init__(self, backbone, net_means, net_logvars, decoder):\n",
    "        super(ResnetVAE, self).__init__()\n",
    "\n",
    "        self.decoder = decoder\n",
    "        self.backbone = backbone\n",
    "        self.net_means = net_means\n",
    "        self.net_logvars = net_logvars\n",
    "        \n",
    "        self.dim_out = net_means[-1].out_features\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.backbone(x)\n",
    "        z_mu = self.net_means(z)\n",
    "\n",
    "        # we predict log_var = log(std**2)\n",
    "        # -> std = exp(0.5 * log_var)\n",
    "        # -> alternative is to directly predict std ;)\n",
    "        z_logvar = self.net_logvars(z)\n",
    "\n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(logvar)\n",
    "        #\n",
    "        return eps * std + mu\n",
    "        #std = logvar.mul(0.5).exp_()\n",
    "        #eps = torch.autograd.Variable(std.data.new(std.size()).normal_())\n",
    "        #eps = torch.randn_like(logvar)\n",
    "        #return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def loss_kld(self, mu, logvar):\n",
    "        loss = 1 + logvar - mu ** 2 - logvar.exp()\n",
    "        loss = -0.5 * torch.sum(loss, dim = 1)\n",
    "        return loss\n",
    "\n",
    "    def loss_rec(self, x1, x2):\n",
    "        #loss = F.mse_loss(x1, x2)\n",
    "        loss = F.mse_loss(x1, x2, reduction='mean')\n",
    "        #loss = F.binary_cross_entropy(x1, x2, reduction='sum')\n",
    "        #loss = ((x1 - x2)**2).mean(axis=1)\n",
    "        #loss = - F.cosine_similarity(x1, x2)\n",
    "        #p = F.normalize(x1, p=2, dim=1)\n",
    "        #z = F.normalize(x2, p=2, dim=1)\n",
    "        #loss = -(p * z).sum(dim=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dl_train, dl_down_train, dl_down_valid = get_dataloaders_from_config(config)\n",
    "#\n",
    "# create model\n",
    "backbone = get_backbone(config.backbone, pretrained=False)\n",
    "\n",
    "# projectors\n",
    "projector_means = get_projector(d_in=backbone.dim_out, **config.projector_args)\n",
    "projector_logvars = get_projector(d_in=backbone.dim_out, **config.projector_args)\n",
    "\n",
    "#\n",
    "decoder = Decoder(d_z=projector_means[-1].out_features, d_out=config.img_size)\n",
    "\n",
    "#\n",
    "model = ResnetVAE(backbone, projector_means, projector_logvars, decoder)\n",
    "\n",
    "# optimizer\n",
    "optimizer = get_optimizer(config.optimizer, model, config.optimizer_args)\n",
    "if config.scheduler is not None:\n",
    "    scheduler = get_scheduler(config.scheduler, optimizer, config.scheduler_args)\n",
    "else:\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "epoch = 0\n",
    "#\n",
    "if P_CKPT is not None:\n",
    "    r = model.load_state_dict(ckpt['model_state_dict'])\n",
    "    print(\"Load model state dict\", r)\n",
    "    if CONTINUE:\n",
    "        print(\"LOAD optimizer\")\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        #\n",
    "        print(\"LOAD scheduler\")\n",
    "        scheduler.load_state_dict(ckpt['lr_scheduler_state_dict'])\n",
    "        #\n",
    "        global_step = ckpt['global_step']\n",
    "        epoch = ckpt['global_epoch']\n",
    "        print(\"Continue epoch {}, step {}\".format(epoch, global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "writer = SummaryWriter(config.p_logs)\n",
    "\n",
    "# create train dir\n",
    "config.p_logs.mkdir(exist_ok=True, parents=True)\n",
    "config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "#\n",
    "print(\"tensorboard --logdir={}\".format(config.p_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUGGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_generator():\n",
    "    for (x1, x2), target in dl_train:\n",
    "        yield x1, x2\n",
    "        \n",
    "generator = dl_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = next(generator)\n",
    "model.train()\n",
    "x1, x2 = x1.to(config.device), x2.to(config.device)\n",
    "optimizer.zero_grad()\n",
    "#\n",
    "mu1, logvar1 = model.encode(x1)\n",
    "z1 = model.reparametrize(mu1, logvar1)\n",
    "#\n",
    "mu2, logvar2 = model.encode(x2)\n",
    "z2 = model.reparametrize(mu2, logvar2)\n",
    "#\n",
    "x1_rec = model.decode(z1)\n",
    "x2_rec = model.decode(z2)\n",
    "#\n",
    "loss_rec1 = model.loss_rec(x1_rec, x1)\n",
    "loss_rec2 = model.loss_rec(x2_rec, x2)\n",
    "#\n",
    "loss_rec = (loss_rec1 + loss_rec2) / 2\n",
    "#\n",
    "## kld loss\n",
    "loss_kld1 = model.loss_kld(mu1, logvar1).mean()\n",
    "loss_kld2 = model.loss_kld(mu2, logvar2).mean()\n",
    "\n",
    "loss_kld = (loss_kld1 + loss_kld2) / 2\n",
    "    \n",
    "loss = (config.loss.lmda_kld * loss_kld) + (config.loss.lmda_rec * loss_rec)\n",
    "        \n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "model = model.to(config.device)\n",
    "for epoch in range(epoch, config.num_epochs, 1):\n",
    "    # STD EVAL\n",
    "    if epoch % config.freqs.std_eval == 0:\n",
    "        std, cov = std_cov_valid(dl_down_valid, model, config.device)\n",
    "        plt.matshow(cov)\n",
    "        plt.colorbar()\n",
    "        print(\"min {:.3f} max: {:.3f}\".format(cov.min(), cov.max()))\n",
    "        plt.show()\n",
    "        #\n",
    "        writer.add_scalar('std', std, global_step)\n",
    "\n",
    "    # STD EVAL BACKBONE\n",
    "    if epoch % config.freqs.std_eval == 0:\n",
    "        std, cov = std_cov_valid(dl_down_valid, model.backbone, config.device)\n",
    "        plt.matshow(cov)\n",
    "        plt.colorbar()\n",
    "        print(\"backbone min {:.3f} max: {:.3f}\".format(cov.min(), cov.max()))\n",
    "        plt.show()\n",
    "        #\n",
    "        writer.add_scalar('std_backbone', std, global_step)\n",
    "        \n",
    "    # KNN EVAL\n",
    "    if epoch % config.freqs.knn_eval == 0:\n",
    "        acc = down_knn(dl_down_valid, model, config.device, n_neighbors=5)\n",
    "        #\n",
    "        writer.add_scalar('acc_knn', acc, global_step)\n",
    "    \n",
    "        # KNN EVAL\n",
    "    if epoch % config.freqs.knn_eval == 0:\n",
    "        acc = down_knn(dl_down_valid, model.backbone, config.device, n_neighbors=5)\n",
    "        #\n",
    "        writer.add_scalar('acc_knn_back', acc, global_step)\n",
    "    \n",
    "    # LINEAR EVAL\n",
    "    if epoch % config.freqs.lin_eval == 0:\n",
    "        classifier = torch.nn.Linear(model.dim_out, config.n_classes).to(config.device)\n",
    "        classifier.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        classifier.bias.data.zero_()\n",
    "        #\n",
    "        criterion = torch.nn.CrossEntropyLoss().to(config.device)\n",
    "        #\n",
    "\n",
    "        optimizer_down = get_optimizer(config.down_optimizer, classifier, config.down_optimizer_args)\n",
    "        scheduler_down = get_scheduler(config.down_scheduler, optimizer_down, config.down_scheduler_args)\n",
    "        #\n",
    "        _, _ = down_train_linear(model, classifier, dl_down_train,\n",
    "                              optimizer_down, config.device, config.down_num_epochs)\n",
    "            \n",
    "        acc = down_valid_linear(\n",
    "                model,\n",
    "                classifier,\n",
    "                dl_down_valid,\n",
    "                config.device)\n",
    "        writer.add_scalar('acc_linear', acc, global_step)\n",
    "    \n",
    "    # REC\n",
    "    if epoch % config.freqs.plot_rec == 0:\n",
    "        model.eval()\n",
    "        x_show, _ = next(iter(dl_down_valid))\n",
    "        x_show = x_show[:10]\n",
    "        with torch.no_grad():\n",
    "            z = model(x_show.to(config.device))\n",
    "            x_rec = model.decode(z)\n",
    "        show_batch(x_show)\n",
    "        show_batch(x_rec.detach().cpu())\n",
    "    \n",
    "    # TRAIN STEP\n",
    "    losses, step = 0., 0.\n",
    "    losses_kld = 0\n",
    "    losses_rec = 0\n",
    "    p_bar = tqdm(dl_train, desc=f'Pretrain {epoch}')\n",
    "    for (x1, x2), target in p_bar:\n",
    "        model.train()\n",
    "        x1, x2 = x1.to(config.device), x2.to(config.device)\n",
    "        optimizer.zero_grad()\n",
    "        #\n",
    "        mu1, logvar1 = model.encode(x1)\n",
    "        z1 = model.reparametrize(mu1, logvar1)\n",
    "        #\n",
    "        mu2, logvar2 = model.encode(x2)\n",
    "        z2 = model.reparametrize(mu2, logvar2)\n",
    "        #\n",
    "        x1_rec = model.decode(z1)\n",
    "        x2_rec = model.decode(z2)\n",
    "        #\n",
    "        loss_rec1 = model.loss_rec(x1_rec, x1)\n",
    "        loss_rec2 = model.loss_rec(x2_rec, x2)\n",
    "        #\n",
    "        loss_rec = (loss_rec1 + loss_rec2) / 2\n",
    "        #\n",
    "        ## kld loss\n",
    "        loss_kld1 = model.loss_kld(mu1, logvar1).mean()\n",
    "        loss_kld2 = model.loss_kld(mu2, logvar2).mean()\n",
    "\n",
    "        loss_kld = (loss_kld1 + loss_kld2) / 2\n",
    "\n",
    "        #loss = loss_rec\n",
    "        #loss = (config.loss.lmda_kld * loss_kld) + (config.loss.lmda_rec * loss_rec)\n",
    "        loss = loss_rec\n",
    "        #loss = loss_rec\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        losses += loss.item()\n",
    "        losses_kld += loss_kld.item()\n",
    "        losses_rec += loss_rec.item()\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "        p_bar.set_postfix({'loss': losses / step, 'rec': losses_rec / step, 'kld': losses_kld / step})\n",
    "        #\n",
    "        writer.add_scalar('loss', loss.item(), global_step)\n",
    "        writer.add_scalar('rec loss', loss_rec.item(), global_step)\n",
    "        writer.add_scalar('kld loss', loss_kld.item(), global_step)\n",
    "        writer.add_scalar('kld1 loss', loss_kld1.item(), global_step)\n",
    "        writer.add_scalar('kld2 loss', loss_kld2.item(), global_step)\n",
    "        #\n",
    "        writer.add_scalar('rec1 loss', loss_rec1.item(), global_step)\n",
    "        writer.add_scalar('rec2 loss', loss_rec2.item(), global_step)\n",
    "\n",
    "        \n",
    "    writer.add_scalar('epoch loss', losses / step, global_step)\n",
    "    \n",
    "    # CHECKPOINTING\n",
    "    if epoch % config.freqs.ckpt == 0 and epoch != 0:\n",
    "        p_ckpt = config.p_ckpts / config.fs_ckpt.format(config.dataset, epoch)\n",
    "        config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "        #\n",
    "        save_checkpoint(model, optimizer, scheduler, config, epoch, global_step, p_ckpt)\n",
    "        print('\\nSave model for epoch {} at {}'.format(epoch, p_ckpt))\n",
    "    writer.add_scalar('epoch', epoch, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
