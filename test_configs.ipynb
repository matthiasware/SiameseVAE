{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "#\n",
    "from dotted_dict import DottedDict\n",
    "import torch\n",
    "#\n",
    "import numpy as np\n",
    "import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_template():\n",
    "    config = DottedDict()\n",
    "    return config\n",
    "\n",
    "def add_paths_to_confg(config):\n",
    "    # run directory name\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fs_run = \"run_{}_{}_{}\".format(config.dataset, config.backbone, timestamp)\n",
    "    \n",
    "    # checkpoint\n",
    "    config.fs_ckpt = \"model_{}_epoch_{:0>6}.ckpt\"\n",
    "    \n",
    "    # train dir\n",
    "    if config.debug:\n",
    "        config.p_train = Path(config.p_base) / \"tmp\" / fs_run\n",
    "    else:\n",
    "        config.p_train = Path(config.p_base) / fs_run\n",
    "    config.p_ckpts = config.p_train / \"ckpts\"\n",
    "    config.p_logs = config.p_train / \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_template()\n",
    "\n",
    "#################\n",
    "# DVICE\n",
    "#################\n",
    "config.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#config.device = 'cpu'\n",
    "\n",
    "#################\n",
    "# frequencies\n",
    "#################\n",
    "config.freqs = {\n",
    "    \"ckpt\": 5,\n",
    "    \"lin_eval\": 5,\n",
    "    \"knn_eval\": 5,\n",
    "    \"std_eval\": 5,\n",
    "    \"plot_cov\": 5,\n",
    "}\n",
    "#################\n",
    "# data\n",
    "#################\n",
    "config.p_data = \"/mnt/data/pytorch\"\n",
    "config.dataset = \"cifar10\"\n",
    "config.img_size = 32\n",
    "config.n_classes = 10\n",
    "config.train_split = 'train'\n",
    "config.valid_split = \"valid\"\n",
    "config.augmentations_train = [\n",
    "    (\"RandomResizedCrop\", {'size': config.img_size, \"scale\": (0.2, 1.0)}),\n",
    "    (\"RandomHorizontalFlip\", {'p': 0.5}),\n",
    "    (\"RandomApply\", {\n",
    "        \"transforms\": [\n",
    "            (\"ColorJitter\", {\"brightness\": 0.4,\n",
    "                             \"contrast\": 0.4,\n",
    "                             \"saturation\": 0.2,\n",
    "                             'hue': 0.1})\n",
    "        ],\n",
    "        \"p\": 0.8,\n",
    "    }),\n",
    "    (\"RandomGrayscale\", {\"p\": 0.1}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#\n",
    "config.augmentations_valid = [\n",
    "    (\"Resize\", {'size': (config.img_size, config.img_size)}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#################\n",
    "# train model\n",
    "#################\n",
    "config.backbone =  \"ResNet-18\"\n",
    "config.projector_args = {\n",
    "    'd_out': 512,\n",
    "    'd_hidden': 512,\n",
    "    'n_hidden': 3,\n",
    "    'normalize': True,\n",
    "    'dropout_rate': None,\n",
    "    'activation_last': False,\n",
    "    'normalize_last': False,\n",
    "    'dropout_rate_last': None,\n",
    "}\n",
    "#################\n",
    "# training\n",
    "#################\n",
    "config.batch_size = 512\n",
    "config.num_epochs = 20\n",
    "config.num_workers = 8\n",
    "\n",
    "#################\n",
    "# optimizer\n",
    "#################\n",
    "config.optimizer = \"sgd\"\n",
    "config.optimizer_args = {\n",
    "        \"lr\": 0.3,\n",
    "        \"weight_decay\": 5e-4,  # used always\n",
    "        \"momentum\": 0.9\n",
    "    }\n",
    "config.scheduler = \"cosine_decay\"\n",
    "config.scheduler_args = {\n",
    "        \"T_max\": config.num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "#################\n",
    "# down train\n",
    "#################\n",
    "config.down_batch_size = 128\n",
    "config.down_num_epochs = 100\n",
    "config.down_num_workers = 8\n",
    "\n",
    "#################\n",
    "# down optimizer\n",
    "#################\n",
    "config.down_optimizer = \"sgd\"\n",
    "config.down_optimizer_args = {\n",
    "        \"lr\": 0.03 * config.down_batch_size / 256,\n",
    "        \"weight_decay\": 5e-4,  # used always\n",
    "        \"momentum\": 0.9\n",
    "    }\n",
    "config.down_scheduler = \"cosine_decay\"\n",
    "config.down_scheduler_args = {\n",
    "        \"T_max\": config.down_num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "\n",
    "config.loss = {\n",
    "    'scale': 0.024,\n",
    "    'lmbda': 0.0051\n",
    "}\n",
    "config.debug = False\n",
    "config.p_base = \"/mnt/experiments/barlow\"\n",
    "add_paths_to_confg(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbones import *\n",
    "from models.projectors import *\n",
    "from models.barlow_twins import BarlowTwins\n",
    "from optimizers import *\n",
    "from augmentations import SimSiamAugmentation, Augmentation\n",
    "from datasets import get_dataset\n",
    "from utils import show, show_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "backbone = get_backbone(config.backbone, zero_init_residual=True)\n",
    "projector = get_projector(d_in=backbone.dim_out, **config.projector_args)\n",
    "model = BarlowTwins(backbone, projector, config.loss[\"scale\"], config.loss[\"lmbda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(config.optimizer, model, config.optimizer_args)\n",
    "scheduler = get_scheduler(config.scheduler, optimizer, config.scheduler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations\n",
    "trans_train = SimSiamAugmentation(config.augmentations_train, downstream=False)\n",
    "trans_down_train = SimSiamAugmentation(config.augmentations_train, downstream=True)\n",
    "trans_down_valid = SimSiamAugmentation(config.augmentations_valid, downstream=True)\n",
    "\n",
    "# Datasets\n",
    "ds_train = get_dataset(\n",
    "    dataset=config.dataset,\n",
    "    p_data=config.p_data,\n",
    "    transform=trans_train,\n",
    "    target_transform=None,\n",
    "    split='train'\n",
    ")\n",
    "ds_down_train = get_dataset(\n",
    "    dataset=config.dataset,\n",
    "    p_data=config.p_data,\n",
    "    transform=trans_down_train,\n",
    "    target_transform=None,\n",
    "    split='train'\n",
    ")\n",
    "ds_down_valid = get_dataset(\n",
    "    dataset=config.dataset,\n",
    "    p_data=config.p_data,\n",
    "    transform=trans_down_train,\n",
    "    target_transform=None,\n",
    "    split='valid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "dl_down_train = DataLoader(\n",
    "    ds_down_train,\n",
    "    batch_size=config.down_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.down_num_workers,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "dl_down_valid = DataLoader(\n",
    "    ds_down_valid,\n",
    "    batch_size=config.down_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.down_num_workers,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, data_loader, model, optimizer, device, debug=False):\n",
    "    model.train()\n",
    "\n",
    "    losses, step = 0., 0.\n",
    "    p_bar = tqdm(data_loader, desc=f'Pretrain {epoch}')\n",
    "    for (x1, x2), target in p_bar:\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x1, x2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        p_bar.set_postfix({'loss': losses / step})\n",
    "        \n",
    "        if debug is True and step == 10:\n",
    "            break\n",
    "\n",
    "    loss_avg = losses / step\n",
    "    return loss_avg, step\n",
    "\n",
    "def train_step(model, optimizer, device, x1, x2):\n",
    "    model.train()\n",
    "    x1, x2 = x1.to(device), x2.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(x1, x2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def knn_eval(epoch, data_loader, model, device, n_neighbors=5):\n",
    "    model.eval()\n",
    "    #\n",
    "    outs = []\n",
    "    targets = []\n",
    "    #\n",
    "    # p_bar = tqdm(data_loader, desc=f'Valid KNN {epoch}')\n",
    "    with tqdm(total=len(data_loader), desc=f'Valid KNN {epoch}') as p_bar:\n",
    "        with torch.no_grad():\n",
    "            for data, target in data_loader:\n",
    "                out = model.backbone(data.to(device)).squeeze()\n",
    "                out = model.projector(out)\n",
    "                outs.append(out.cpu().numpy())\n",
    "                targets.append(target.cpu().numpy())\n",
    "\n",
    "                p_bar.update()\n",
    "\n",
    "        x = np.concatenate(outs)\n",
    "        y = np.concatenate(targets)\n",
    "\n",
    "        neigh = KNeighborsClassifier(n_neighbors=n_neighbors,\n",
    "                                     algorithm='brute', n_jobs=8)\n",
    "        neigh.fit(x, y)\n",
    "        score = neigh.score(x, y)\n",
    "\n",
    "        p_bar.set_postfix({\"acc\": score})\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "epoch = 0\n",
    "model = model.to(config.device)\n",
    "for epoch in range(epoch, config.num_epochs, 1):\n",
    "    score = knn_eval(epoch, dl_down_valid, model, config.device)\n",
    "    print(score)\n",
    "    \n",
    "    losses, step = 0., 0.\n",
    "    p_bar = tqdm(dl_train, desc=f'Pretrain {epoch}')\n",
    "    for (x1, x2), target in p_bar:\n",
    "        loss = train_step(model, optimizer, config.device, x1, x2)\n",
    "        losses += loss.item()\n",
    "        step += 1\n",
    "        global_step += 1\n",
    "        p_bar.set_postfix({'loss': losses / step}) \n",
    "    # knn eval\n",
    "    \n",
    "    # std eval\n",
    "\n",
    "    # linear train\n",
    "    # linear eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
