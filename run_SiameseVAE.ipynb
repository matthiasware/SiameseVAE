{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "#\n",
    "from dotted_dict import DottedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#\n",
    "import numpy as np\n",
    "import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbones import *\n",
    "from models.projectors import *\n",
    "from models.barlow_twins import BarlowTwins\n",
    "from optimizers import *\n",
    "from augmentations import SimSiamAugmentation, Augmentation\n",
    "from datasets import get_dataset\n",
    "from utils import show, show_batch, save_checkpoint\n",
    "from config_utils import get_dataloaders_from_config, get_config_template, add_paths_to_confg\n",
    "from train_utils import down_knn, down_train_linear, down_valid_linear, std_cov_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_template()\n",
    "\n",
    "#################\n",
    "# DVICE\n",
    "#################\n",
    "config.device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#################\n",
    "# frequencies\n",
    "#################\n",
    "config.freqs = {\n",
    "    \"ckpt\": 20,\n",
    "    \"lin_eval\": 1,\n",
    "    \"knn_eval\": 1,\n",
    "    \"std_eval\": 1,\n",
    "}\n",
    "#################\n",
    "# data\n",
    "#################\n",
    "config.p_data = \"/mnt/data/pytorch\"\n",
    "config.dataset = \"cifar10\"\n",
    "config.img_size = 32\n",
    "config.n_classes = 10\n",
    "config.train_split = 'train'\n",
    "config.down_train_split = 'train'\n",
    "config.down_valid_split = \"valid\"\n",
    "config.augmentations_train = [\n",
    "    (\"RandomResizedCrop\", {'size': config.img_size, \"scale\": (0.2, 1.0)}),\n",
    "    (\"RandomHorizontalFlip\", {'p': 0.5}),\n",
    "    (\"RandomApply\", {\n",
    "        \"transforms\": [\n",
    "            (\"ColorJitter\", {\"brightness\": 0.3,\n",
    "                             \"contrast\": 0.3,\n",
    "                             \"saturation\": 0.1,\n",
    "                             'hue': 0.1})\n",
    "        ],\n",
    "        \"p\": 0.5,\n",
    "    }),\n",
    "    (\"RandomGrayscale\", {\"p\": 0.1}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#\n",
    "config.augmentations_valid = [\n",
    "    (\"Resize\", {'size': (config.img_size, config.img_size)}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#################\n",
    "# train model\n",
    "#################\n",
    "config.backbone =  \"MobileNet-v3-Small\"\n",
    "config.projector_args = {\n",
    "    'd_out': 512,\n",
    "    'd_hidden': 512,\n",
    "    'n_hidden': 0,\n",
    "    'normalize': False,\n",
    "    'dropout_rate': 0.05,\n",
    "    'activation_last': False,\n",
    "    'normalize_last': False,\n",
    "    'dropout_rate_last': None,\n",
    "}\n",
    "#################\n",
    "# training\n",
    "#################\n",
    "config.batch_size = 1024\n",
    "config.num_epochs = 400\n",
    "config.num_workers = 8\n",
    "\n",
    "#################\n",
    "# optimizer\n",
    "#################\n",
    "config.optimizer = \"sgd\"\n",
    "config.optimizer_args = {\n",
    "        \"lr\": 1e-2,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"momentum\": 0.9\n",
    "    }\n",
    "config.scheduler = \"cosine_decay\"\n",
    "config.scheduler_args = {\n",
    "        \"T_max\": config.num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "#################\n",
    "# down train\n",
    "#################\n",
    "config.down_batch_size = 512\n",
    "config.down_num_epochs = 1\n",
    "config.down_num_workers = 8\n",
    "\n",
    "#################\n",
    "# down optimizer\n",
    "#################\n",
    "config.down_optimizer = \"sgd\"\n",
    "config.down_optimizer_args = {\n",
    "        \"lr\": 0.03 * config.down_batch_size / 256,\n",
    "        \"weight_decay\": 5e-4,  # used always\n",
    "        \"momentum\": 0.9\n",
    "    }\n",
    "config.down_scheduler = \"cosine_decay\"\n",
    "config.down_scheduler_args = {\n",
    "        \"T_max\": config.down_num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "\n",
    "config.loss = {\n",
    "    'lmda_rec': 10,\n",
    "    'lmda_kld': 1\n",
    "}\n",
    "config.debug = False\n",
    "config.p_base = \"/mnt/experiments/siamesevae\"\n",
    "add_paths_to_confg(config)\n",
    "config = DottedDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META VARS\n",
    "P_CKPT = None\n",
    "CONTINUE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P_CKPT is not None:\n",
    "    print(\"LOADING CHECKPOINT {}\".format(P_CKPT))\n",
    "    ckpt = torch.load(P_CKPT)\n",
    "    \n",
    "    if CONTINUE:\n",
    "        print(\"USING CKPT Config\")\n",
    "        config = ckpt[\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseVAE(torch.nn.Module):\n",
    "    def __init__(self, backbone, net_means, net_logvars):\n",
    "        super(SiameseVAE, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.net_means = net_means\n",
    "        self.net_logvars = net_logvars\n",
    "        \n",
    "        self.dim_out = net_means[-1].out_features\n",
    "\n",
    "    def encode(self, x):\n",
    "        inter = self.backbone(x)\n",
    "        z_mu = self.net_means(inter)\n",
    "\n",
    "        # we predict log_var = log(std**2)\n",
    "        # -> std = exp(0.5 * log_var)\n",
    "        # -> alternative is to directly predict std ;)\n",
    "        z_logvar = self.net_logvars(inter)\n",
    "\n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(logvar)\n",
    "        #\n",
    "        return eps * std + mu\n",
    "        \n",
    "    def loss_kld(self, mu, logvar):\n",
    "        loss = 1 + logvar - mu ** 2 - logvar.exp()\n",
    "        loss = -0.5 * torch.sum(loss, dim = 1)\n",
    "        return loss\n",
    "\n",
    "    def loss_rec(self, x1, x2):\n",
    "        #loss = F.mse_loss(x1, x2)\n",
    "        loss = ((x1 - x2)**2).mean(axis=1)\n",
    "        #loss = - F.cosine_similarity(x1, x2)\n",
    "        #p = F.normalize(x1, p=2, dim=1)\n",
    "        #z = F.normalize(x2, p=2, dim=1)\n",
    "        #loss = -(p * z).sum(dim=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dl_train, dl_down_train, dl_down_valid = get_dataloaders_from_config(config)\n",
    "#\n",
    "# create model\n",
    "backbone = get_backbone(config.backbone, pretrained=False)\n",
    "\n",
    "# projectors\n",
    "projector_means = get_projector(d_in=backbone.dim_out, **config.projector_args)\n",
    "projector_logvars = get_projector(d_in=backbone.dim_out, **config.projector_args)\n",
    "\n",
    "#\n",
    "model = SiameseVAE(backbone, projector_means, projector_logvars)\n",
    "\n",
    "# optimizer\n",
    "optimizer = get_optimizer(config.optimizer, model, config.optimizer_args)\n",
    "scheduler = get_scheduler(config.scheduler, optimizer, config.scheduler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "epoch = 0\n",
    "#\n",
    "if P_CKPT is not None:\n",
    "    r = model.load_state_dict(ckpt['model_state_dict'])\n",
    "    print(\"Load model state dict\", r)\n",
    "    if CONTINUE:\n",
    "        print(\"LOAD optimizer\")\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        #\n",
    "        print(\"LOAD scheduler\")\n",
    "        scheduler.load_state_dict(ckpt['lr_scheduler_state_dict'])\n",
    "        #\n",
    "        global_step = ckpt['global_step']\n",
    "        epoch = ckpt['global_epoch']\n",
    "        print(\"Continue epoch {}, step {}\".format(epoch, global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "writer = SummaryWriter(config.p_logs)\n",
    "\n",
    "# create train dir\n",
    "config.p_logs.mkdir(exist_ok=True, parents=True)\n",
    "config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "#\n",
    "print(\"tensorboard --logdir={}\".format(config.p_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUGGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_generator():\n",
    "    for (x1, x2), target in dl_train:\n",
    "        yield x1, x2\n",
    "        \n",
    "generator = dl_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1, x2 = next(generator)\n",
    "\n",
    "model.train()\n",
    "x1, x2 = x1.to(config.device), x2.to(config.device)\n",
    "optimizer.zero_grad()\n",
    "    \n",
    "mu1, logvar1 = model.encode(x1)\n",
    "mu2, logvar2 = model.encode(x2)\n",
    "    \n",
    "z1 = model.reparametrize(mu1, logvar1)\n",
    "z2 = model.reparametrize(mu1, logvar2)\n",
    "    \n",
    "# rec loss\n",
    "loss_rec = model.loss_rec(z1, z2).mean()\n",
    "    \n",
    "# kld loss\n",
    "loss_kld1 = model.loss_kld(mu1, logvar1).mean()\n",
    "loss_kld2 = model.loss_kld(mu2, logvar2).mean()\n",
    "    \n",
    "loss_kld = (loss_kld1 + loss_kld2) / 2\n",
    "    \n",
    "loss = (config.loss.lmda_kld * loss_kld) + (config.loss.lmda_rec * loss_rec)\n",
    "    \n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "        \n",
    "print(\"rec {} kld {} kld_1 {} kld_2 {}\".format(loss_rec.item(), loss_kld.item(), loss_kld1.item(), loss_kld2.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = model.to(config.device)\n",
    "for epoch in range(epoch, config.num_epochs, 1):\n",
    "    # STD EVAL\n",
    "    if epoch % config.freqs.std_eval == 0:\n",
    "        std, cov = std_cov_valid(dl_down_valid, model, config.device)\n",
    "        plt.matshow(cov)\n",
    "        plt.colorbar()\n",
    "        print(\"min {:.3f} max: {:.3f}\".format(cov.min(), cov.max()))\n",
    "        plt.show()\n",
    "        #\n",
    "        writer.add_scalar('std', std, global_step)\n",
    "\n",
    "    # STD EVAL BACKBONE\n",
    "    if epoch % config.freqs.std_eval == 0:\n",
    "        std, cov = std_cov_valid(dl_down_valid, model.backbone, config.device)\n",
    "        plt.matshow(cov)\n",
    "        plt.colorbar()\n",
    "        print(\"backbone min {:.3f} max: {:.3f}\".format(cov.min(), cov.max()))\n",
    "        plt.show()\n",
    "        #\n",
    "        writer.add_scalar('std_backbone', std, global_step)\n",
    "        \n",
    "    # KNN EVAL\n",
    "    if epoch % config.freqs.knn_eval == 0:\n",
    "        acc = down_knn(dl_down_valid, model, config.device, n_neighbors=5)\n",
    "        #\n",
    "        writer.add_scalar('acc_knn', acc, global_step)\n",
    "    \n",
    "        # KNN EVAL\n",
    "    if epoch % config.freqs.knn_eval == 0:\n",
    "        acc = down_knn(dl_down_valid, model.backbone, config.device, n_neighbors=5)\n",
    "        #\n",
    "        writer.add_scalar('acc_knn_back', acc, global_step)\n",
    "    \n",
    "    # LINEAR EVAL\n",
    "    if epoch % config.freqs.lin_eval == 0:\n",
    "        classifier = torch.nn.Linear(model.dim_out, config.n_classes).to(config.device)\n",
    "        classifier.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        classifier.bias.data.zero_()\n",
    "        #\n",
    "        criterion = torch.nn.CrossEntropyLoss().to(config.device)\n",
    "        #\n",
    "\n",
    "        optimizer_down = get_optimizer(config.down_optimizer, classifier, config.down_optimizer_args)\n",
    "        scheduler_down = get_scheduler(config.down_scheduler, optimizer_down, config.down_scheduler_args)\n",
    "        #\n",
    "        _, _ = down_train_linear(model, classifier, dl_down_train,\n",
    "                              optimizer_down, config.device, config.down_num_epochs)\n",
    "            \n",
    "        acc = down_valid_linear(\n",
    "                model,\n",
    "                classifier,\n",
    "                dl_down_valid,\n",
    "                config.device)\n",
    "        writer.add_scalar('acc_linear', acc, global_step)\n",
    "    \n",
    "    # TRAIN STEP\n",
    "    losses, step = 0., 0.\n",
    "    losses_kld = 0\n",
    "    losses_rec = 0\n",
    "    p_bar = tqdm(dl_train, desc=f'Pretrain {epoch}')\n",
    "    for (x1, x2), target in p_bar:\n",
    "        model.train()\n",
    "        x1, x2 = x1.to(config.device), x2.to(config.device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        mu1, logvar1 = model.encode(x1)\n",
    "        z1 = model.reparametrize(mu1, logvar1)\n",
    "        \n",
    "        mu2, logvar2 = model.encode(x2)\n",
    "        z2 = model.reparametrize(mu2, logvar2)\n",
    "    \n",
    "        # rec loss\n",
    "        loss_rec = model.loss_rec(z1, z2).mean()\n",
    "        loss = loss_rec\n",
    "    \n",
    "        ## kld loss\n",
    "        #loss_kld1 = model.loss_kld(mu1, logvar1).mean()\n",
    "        #loss_kld2 = model.loss_kld(mu2, logvar2).mean()\n",
    "        loss_kld1 = torch.Tensor([0])\n",
    "        loss_kld2 = torch.Tensor([0])\n",
    "    \n",
    "        loss_kld = (loss_kld1 + loss_kld2) / 2\n",
    "    \n",
    "        #loss = (config.loss.lmda_kld * loss_kld) + (config.loss.lmda_rec * loss_rec)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        losses += loss.item()\n",
    "        losses_kld += loss_kld.item()\n",
    "        losses_rec += loss_rec.item()\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "        p_bar.set_postfix({'loss': losses / step, 'rec': losses_rec / step, 'kld': losses_kld / step})\n",
    "        #\n",
    "        writer.add_scalar('loss', loss.item(), global_step)\n",
    "        writer.add_scalar('rec loss', loss_rec.item(), global_step)\n",
    "        writer.add_scalar('kld loss', loss_kld.item(), global_step)\n",
    "        writer.add_scalar('kld1 loss', loss_kld1.item(), global_step)\n",
    "        writer.add_scalar('kld2 loss', loss_kld2.item(), global_step)\n",
    "\n",
    "        \n",
    "    writer.add_scalar('epoch loss', losses / step, global_step)\n",
    "    \n",
    "    # CHECKPOINTING\n",
    "    if epoch % config.freqs.ckpt == 0 and epoch != 0:\n",
    "        p_ckpt = config.p_ckpts / config.fs_ckpt.format(config.dataset, epoch)\n",
    "        config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "        #\n",
    "        save_checkpoint(model, optimizer, scheduler, config, epoch, global_step, p_ckpt)\n",
    "        print('\\nSave model for epoch {} at {}'.format(epoch, p_ckpt))\n",
    "    writer.add_scalar('epoch', epoch, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
