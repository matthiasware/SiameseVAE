{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "#\n",
    "from dotted_dict import DottedDict\n",
    "import torch\n",
    "#\n",
    "import numpy as np\n",
    "import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbones import *\n",
    "from models.projectors import *\n",
    "from models.barlow_twins import BarlowTwins\n",
    "from optimizers import *\n",
    "from augmentations import SimSiamAugmentation, Augmentation\n",
    "from datasets import get_dataset\n",
    "from utils import show, show_batch, save_checkpoint\n",
    "from config_utils import get_dataloaders_from_config, get_config_template, add_paths_to_confg\n",
    "from train_utils import down_knn, down_train_linear, down_valid_linear, std_cov_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_template()\n",
    "\n",
    "#################\n",
    "# DVICE\n",
    "#################\n",
    "config.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#config.device = 'cpu'\n",
    "\n",
    "#################\n",
    "# frequencies\n",
    "#################\n",
    "config.freqs = {\n",
    "    \"ckpt\": 10,\n",
    "    \"lin_eval\": 5,\n",
    "    \"knn_eval\": 5,\n",
    "    \"std_eval\": 5,\n",
    "}\n",
    "#################\n",
    "# data\n",
    "#################\n",
    "config.p_data = \"/mnt/data/pytorch\"\n",
    "config.dataset = \"cifar10\"\n",
    "config.img_size = 64\n",
    "config.n_classes = 10\n",
    "config.train_split = 'train'\n",
    "config.down_train_split = 'train'\n",
    "config.down_valid_split = \"valid\"\n",
    "config.augmentations_train = [\n",
    "    (\"RandomResizedCrop\", {'size': config.img_size, \"scale\": (0.2, 1.0)}),\n",
    "    (\"RandomHorizontalFlip\", {'p': 0.5}),\n",
    "    (\"RandomApply\", {\n",
    "        \"transforms\": [\n",
    "            (\"ColorJitter\", {\"brightness\": 0.4,\n",
    "                             \"contrast\": 0.4,\n",
    "                             \"saturation\": 0.2,\n",
    "                             'hue': 0.1})\n",
    "        ],\n",
    "        \"p\": 0.8,\n",
    "    }),\n",
    "    (\"RandomApply\", {\n",
    "        \"transforms\": [\n",
    "            ('GaussianBlur', {\n",
    "             'kernel_size': 128 // 20 * 2 + 1, 'sigma': (0.5, 2.0)})\n",
    "        ],\n",
    "        \"p\": 0.9,\n",
    "    }),\n",
    "    (\"RandomGrayscale\", {\"p\": 0.1}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#\n",
    "config.augmentations_valid = [\n",
    "    (\"Resize\", {'size': (config.img_size, config.img_size)}),\n",
    "    (\"ToTensor\", {}),\n",
    "    ('Normalize', {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std':[0.229, 0.224, 0.225]}),\n",
    "]\n",
    "#################\n",
    "# train model\n",
    "#################\n",
    "config.backbone =  \"ResNet-18\"\n",
    "config.projector_args = {\n",
    "    'd_out': 2048,\n",
    "    'd_hidden': 2048,\n",
    "    'n_hidden': 2,\n",
    "    'normalize': True,\n",
    "    'dropout_rate': None,\n",
    "    'activation_last': False,\n",
    "    'normalize_last': False,\n",
    "    'dropout_rate_last': None,\n",
    "}\n",
    "#################\n",
    "# training\n",
    "#################\n",
    "config.batch_size = 512\n",
    "config.num_epochs = 1600\n",
    "config.num_workers = 8\n",
    "\n",
    "#################\n",
    "# optimizer\n",
    "#################\n",
    "config.optimizer = \"sgd\"\n",
    "config.optimizer_args = {\n",
    "        \"lr\": 0.6,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"momentum\": 0.90\n",
    "    }\n",
    "config.scheduler = \"cosine_decay\"\n",
    "config.scheduler_args = {\n",
    "        \"T_max\": config.num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "#################\n",
    "# down train\n",
    "#################\n",
    "config.down_batch_size = 128\n",
    "config.down_num_epochs = 2\n",
    "config.down_num_workers = 8\n",
    "\n",
    "#################\n",
    "# down optimizer\n",
    "#################\n",
    "config.down_optimizer = \"sgd\"\n",
    "config.down_optimizer_args = {\n",
    "        \"lr\": 0.03 * config.down_batch_size / 256,\n",
    "        \"weight_decay\": 5e-4,  # used always\n",
    "        \"momentum\": 0.9\n",
    "    }\n",
    "config.down_scheduler = \"cosine_decay\"\n",
    "config.down_scheduler_args = {\n",
    "        \"T_max\": config.down_num_epochs,\n",
    "        \"eta_min\": 0,\n",
    "}\n",
    "\n",
    "config.loss = {\n",
    "    'scale': 0.024,\n",
    "    'lmbda': 0.0051\n",
    "}\n",
    "config.debug = False\n",
    "config.p_base = \"/mnt/experiments/barlow\"\n",
    "add_paths_to_confg(config)\n",
    "config = DottedDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META VARS\n",
    "P_CKPT = None\n",
    "CONTINUE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P_CKPT is not None:\n",
    "    print(\"LOADING CHECKPOINT {}\".format(P_CKPT))\n",
    "    ckpt = torch.load(P_CKPT)\n",
    "    \n",
    "    if CONTINUE:\n",
    "        print(\"USING CKPT Config\")\n",
    "        config = ckpt[\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "backbone = get_backbone(config.backbone, zero_init_residual=True, pretrained=False)\n",
    "projector = get_projector(d_in=backbone.dim_out, **config.projector_args)\n",
    "model = BarlowTwins(backbone, projector, config.loss[\"scale\"], config.loss[\"lmbda\"])\n",
    "model = model.to(config.device) # important to put model already to device, otherwise optimizer fails! (BUG)\n",
    "\n",
    "# load data\n",
    "dl_train, dl_down_train, dl_down_valid = get_dataloaders_from_config(config)\n",
    "\n",
    "# optimizer\n",
    "optimizer = get_optimizer(config.optimizer, model, config.optimizer_args)\n",
    "scheduler = get_scheduler(config.scheduler, optimizer, config.scheduler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "epoch = 0\n",
    "#\n",
    "if P_CKPT is not None:\n",
    "    r = model.load_state_dict(ckpt['model_state_dict'])\n",
    "    print(\"Load model state dict\", r)\n",
    "    if CONTINUE:\n",
    "        print(\"LOAD optimizer\")\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        #\n",
    "        print(\"LOAD scheduler\")\n",
    "        scheduler.load_state_dict(ckpt['lr_scheduler_state_dict'])\n",
    "        #\n",
    "        global_step = ckpt['global_step']\n",
    "        epoch = ckpt['global_epoch']\n",
    "        print(\"Continue epoch {}, step {}\".format(epoch, global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, device, x1, x2):\n",
    "    model.train()\n",
    "    x1, x2 = x1.to(device), x2.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(x1, x2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "writer = SummaryWriter(config.p_logs)\n",
    "\n",
    "# create train dir\n",
    "config.p_logs.mkdir(exist_ok=True, parents=True)\n",
    "config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "#\n",
    "print(\"tensorboard --logdir={}\".format(config.p_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = model.to(config.device)\n",
    "for epoch in range(epoch, config.num_epochs, 1):\n",
    "    # STD EVAL\n",
    "    if epoch % config.freqs.std_eval == 0:\n",
    "        std, cov = std_cov_valid(dl_down_valid, model.backbone, config.device)\n",
    "        plt.matshow(cov)\n",
    "        plt.colorbar()\n",
    "        print(\"min {:.3f} max: {:.3f}\".format(cov.min(), cov.max()))\n",
    "        plt.show()\n",
    "        #\n",
    "        writer.add_scalar('std', std, global_step)\n",
    "    \n",
    "    # KNN EVAL\n",
    "    if epoch % config.freqs.knn_eval == 0:\n",
    "        acc = down_knn(dl_down_valid, model.backbone, config.device, n_neighbors=5)\n",
    "        #\n",
    "        writer.add_scalar('acc_knn', acc, global_step)\n",
    "    \n",
    "    # LINEAR EVAL\n",
    "    if epoch % config.freqs.lin_eval == 0:\n",
    "        classifier = torch.nn.Linear(model.backbone.dim_out, config.n_classes).to(config.device)\n",
    "        classifier.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        classifier.bias.data.zero_()\n",
    "        #\n",
    "        criterion = torch.nn.CrossEntropyLoss().to(config.device)\n",
    "        #\n",
    "\n",
    "        optimizer_down = get_optimizer(config.down_optimizer, classifier, config.down_optimizer_args)\n",
    "        scheduler_down = get_scheduler(config.down_scheduler, optimizer_down, config.down_scheduler_args)\n",
    "        #\n",
    "        _, _ = down_train_linear(model.backbone, classifier, dl_down_train,\n",
    "                              optimizer_down, config.device, config.down_num_epochs)\n",
    "            \n",
    "        acc = down_valid_linear(\n",
    "                model.backbone,\n",
    "                classifier,\n",
    "                dl_down_valid,\n",
    "                config.device)\n",
    "        writer.add_scalar('acc_linear', acc, global_step)\n",
    "    \n",
    "    # TRAIN STEP\n",
    "    losses, step = 0., 0.\n",
    "    p_bar = tqdm(dl_train, desc=f'Pretrain {epoch}')\n",
    "    for (x1, x2), target in p_bar:\n",
    "        loss = train_step(model, optimizer, config.device, x1, x2)\n",
    "        losses += loss.item()\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "        p_bar.set_postfix({'loss': losses / step})\n",
    "        #\n",
    "        writer.add_scalar('batch loss', loss.item(), global_step)\n",
    "    \n",
    "    writer.add_scalar('epoch loss', losses / step, global_step)\n",
    "    \n",
    "    # CHECKPOINTING\n",
    "    if epoch % config.freqs.ckpt == 0 and epoch != 0:\n",
    "        p_ckpt = config.p_ckpts / config.fs_ckpt.format(config.dataset, epoch)\n",
    "        config.p_ckpts.mkdir(exist_ok=True, parents=True)\n",
    "        #\n",
    "        save_checkpoint(model, optimizer, scheduler, config, epoch, global_step, p_ckpt)\n",
    "        print('\\nSave model for epoch {} at {}'.format(epoch, p_ckpt))\n",
    "    writer.add_scalar('epoch', epoch, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and export representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: 1000, 5000, 10000, 50000, 100000\n",
    "n_samples = len(dl_down_train) * config.down_batch_size\n",
    "\n",
    "max_imgs = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_features_base = config.p_train / \"representations\"\n",
    "p_features_base.mkdir(exist_ok=True)\n",
    "p_imgs = p_features_base / f'X_{n_samples}.npy'\n",
    "p_features = p_features_base / f\"R_{n_samples}.npy\"\n",
    "p_targets = p_features_base / f\"Y_{n_samples}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dl_down_train) * config.down_batch_size)\n",
    "print(len(dl_down_valid) *  config.down_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_final = SimSiamAugmentation(config.augmentations_valid, downstream=True)\n",
    "ds_final = get_dataset(\n",
    "            dataset=config.dataset,\n",
    "            p_data=config.p_data,\n",
    "            transform=trans_final,\n",
    "            target_transform=None,\n",
    "            split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dl_final = DataLoader(\n",
    "        ds_final,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        drop_last=False,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(dl_final)\n",
    "max_imgs = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_imgs = []\n",
    "all_targets = []\n",
    "\n",
    "all_samples = 0\n",
    "model.eval()\n",
    "for x, y in dl_final:\n",
    "    with torch.no_grad():\n",
    "        r = model.backbone(x.to(config.device))\n",
    "        #\n",
    "        r = r.detach().cpu().numpy()\n",
    "        x = x.detach().cpu().numpy()\n",
    "        y = y.detach().cpu().numpy()\n",
    "        #\n",
    "        all_features.append(r)\n",
    "        if n_samples <= max_imgs:\n",
    "            all_imgs.append(x)\n",
    "        all_targets.append(y)\n",
    "        #\n",
    "        all_samples += x.shape[0]\n",
    "    if all_samples % 1000 == 0:\n",
    "        print(all_samples)\n",
    "    if all_samples >= n_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.concatenate(all_features)\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_samples <= max_imgs:\n",
    "    X = np.concatenate(all_imgs)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate(all_targets)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_samples <= max_imgs:\n",
    "    np.save(p_imgs, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(p_features, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(p_targets, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
